{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      " 32  Unnamed: 32              0 non-null      float64\n",
      "dtypes: float64(31), int64(1), object(1)\n",
      "memory usage: 146.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   diagnosis                569 non-null    object \n",
      " 1   radius_mean              569 non-null    float64\n",
      " 2   texture_mean             569 non-null    float64\n",
      " 3   perimeter_mean           569 non-null    float64\n",
      " 4   area_mean                569 non-null    float64\n",
      " 5   smoothness_mean          569 non-null    float64\n",
      " 6   compactness_mean         569 non-null    float64\n",
      " 7   concavity_mean           569 non-null    float64\n",
      " 8   concave points_mean      569 non-null    float64\n",
      " 9   symmetry_mean            569 non-null    float64\n",
      " 10  fractal_dimension_mean   569 non-null    float64\n",
      " 11  radius_se                569 non-null    float64\n",
      " 12  texture_se               569 non-null    float64\n",
      " 13  perimeter_se             569 non-null    float64\n",
      " 14  area_se                  569 non-null    float64\n",
      " 15  smoothness_se            569 non-null    float64\n",
      " 16  compactness_se           569 non-null    float64\n",
      " 17  concavity_se             569 non-null    float64\n",
      " 18  concave points_se        569 non-null    float64\n",
      " 19  symmetry_se              569 non-null    float64\n",
      " 20  fractal_dimension_se     569 non-null    float64\n",
      " 21  radius_worst             569 non-null    float64\n",
      " 22  texture_worst            569 non-null    float64\n",
      " 23  perimeter_worst          569 non-null    float64\n",
      " 24  area_worst               569 non-null    float64\n",
      " 25  smoothness_worst         569 non-null    float64\n",
      " 26  compactness_worst        569 non-null    float64\n",
      " 27  concavity_worst          569 non-null    float64\n",
      " 28  concave points_worst     569 non-null    float64\n",
      " 29  symmetry_worst           569 non-null    float64\n",
      " 30  fractal_dimension_worst  569 non-null    float64\n",
      "dtypes: float64(30), object(1)\n",
      "memory usage: 137.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts diagnosis value of M and B to 0 or 1 to make it easy for skitlearn\n",
    "data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['diagnosis'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['diagnosis'].values # makes y be an num array \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = data.drop(['diagnosis'], axis=1) \n",
    "# drops diagonosis from the list so u can have all the variables needed\n",
    "# for prediction on its own\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data – x_data.min(): \n",
    "# subtracts the minimum value from each value in the dataset \n",
    "# shifting the data so that the smallest value becomes 0.\n",
    "# \n",
    "# x_data.max() – x_data.min(): \n",
    "# calculates the range of the data (difference between the \n",
    "# maximum and minimum values).\n",
    "# \n",
    "\n",
    "x = (x_data - x_data.min()) / (x_data.max() - x_data.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the data between training and testing data\n",
    "# it does a 15% split for the test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.15, random_state = 42) # cs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This transposes the matrix so that each \n",
    "# column is one training example, and \n",
    "# each row is a feature, which aligns \n",
    "# with how matrix multiplication in \n",
    "# logistic regression is designed.\n",
    "\n",
    "# to ensure that the data has the correct\n",
    "# shape for matrix operations during the\n",
    "# logistic regression.\n",
    "\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train:  (30, 483)\n",
      "x test:  (30, 86)\n",
      "y train:  (483,)\n",
      "y test:  (86,)\n"
     ]
    }
   ],
   "source": [
    "print(\"x train: \", x_train.shape)\n",
    "print(\"x test: \", x_test.shape)\n",
    "print(\"y train: \", y_train.shape)\n",
    "print(\"y test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>9</th>\n",
       "      <th>468</th>\n",
       "      <th>382</th>\n",
       "      <th>322</th>\n",
       "      <th>84</th>\n",
       "      <th>526</th>\n",
       "      <th>500</th>\n",
       "      <th>561</th>\n",
       "      <th>332</th>\n",
       "      <th>110</th>\n",
       "      <th>...</th>\n",
       "      <th>330</th>\n",
       "      <th>214</th>\n",
       "      <th>466</th>\n",
       "      <th>121</th>\n",
       "      <th>20</th>\n",
       "      <th>71</th>\n",
       "      <th>106</th>\n",
       "      <th>270</th>\n",
       "      <th>435</th>\n",
       "      <th>102</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>radius_mean</th>\n",
       "      <td>0.259312</td>\n",
       "      <td>0.502579</td>\n",
       "      <td>0.239907</td>\n",
       "      <td>0.278243</td>\n",
       "      <td>0.237541</td>\n",
       "      <td>0.306640</td>\n",
       "      <td>0.381419</td>\n",
       "      <td>0.199678</td>\n",
       "      <td>0.200625</td>\n",
       "      <td>0.132330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428274</td>\n",
       "      <td>0.341190</td>\n",
       "      <td>0.291495</td>\n",
       "      <td>0.552747</td>\n",
       "      <td>0.288655</td>\n",
       "      <td>0.090255</td>\n",
       "      <td>0.220503</td>\n",
       "      <td>0.345923</td>\n",
       "      <td>0.331251</td>\n",
       "      <td>0.246060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture_mean</th>\n",
       "      <td>0.484613</td>\n",
       "      <td>0.460602</td>\n",
       "      <td>0.439973</td>\n",
       "      <td>0.122083</td>\n",
       "      <td>0.200879</td>\n",
       "      <td>0.305715</td>\n",
       "      <td>0.237741</td>\n",
       "      <td>0.664863</td>\n",
       "      <td>0.343253</td>\n",
       "      <td>0.246195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196145</td>\n",
       "      <td>0.476835</td>\n",
       "      <td>0.373013</td>\n",
       "      <td>0.250592</td>\n",
       "      <td>0.202908</td>\n",
       "      <td>0.166723</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>0.240446</td>\n",
       "      <td>0.335137</td>\n",
       "      <td>0.365573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter_mean</th>\n",
       "      <td>0.277659</td>\n",
       "      <td>0.519729</td>\n",
       "      <td>0.241587</td>\n",
       "      <td>0.269712</td>\n",
       "      <td>0.229148</td>\n",
       "      <td>0.301638</td>\n",
       "      <td>0.379656</td>\n",
       "      <td>0.185751</td>\n",
       "      <td>0.194527</td>\n",
       "      <td>0.129293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428512</td>\n",
       "      <td>0.339161</td>\n",
       "      <td>0.291549</td>\n",
       "      <td>0.536314</td>\n",
       "      <td>0.289130</td>\n",
       "      <td>0.103656</td>\n",
       "      <td>0.216847</td>\n",
       "      <td>0.321401</td>\n",
       "      <td>0.327068</td>\n",
       "      <td>0.231014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_mean</th>\n",
       "      <td>0.140997</td>\n",
       "      <td>0.355037</td>\n",
       "      <td>0.129077</td>\n",
       "      <td>0.153256</td>\n",
       "      <td>0.127169</td>\n",
       "      <td>0.172895</td>\n",
       "      <td>0.231559</td>\n",
       "      <td>0.102863</td>\n",
       "      <td>0.103415</td>\n",
       "      <td>0.062227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275589</td>\n",
       "      <td>0.198176</td>\n",
       "      <td>0.166872</td>\n",
       "      <td>0.395970</td>\n",
       "      <td>0.159703</td>\n",
       "      <td>0.042630</td>\n",
       "      <td>0.114104</td>\n",
       "      <td>0.207466</td>\n",
       "      <td>0.193425</td>\n",
       "      <td>0.133701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness_mean</th>\n",
       "      <td>0.595558</td>\n",
       "      <td>0.363456</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.548614</td>\n",
       "      <td>0.402636</td>\n",
       "      <td>0.495351</td>\n",
       "      <td>0.417080</td>\n",
       "      <td>0.197346</td>\n",
       "      <td>0.476393</td>\n",
       "      <td>0.461045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.381692</td>\n",
       "      <td>0.379164</td>\n",
       "      <td>0.308026</td>\n",
       "      <td>0.476393</td>\n",
       "      <td>0.495351</td>\n",
       "      <td>0.408053</td>\n",
       "      <td>0.555836</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.481809</td>\n",
       "      <td>0.248262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness_mean</th>\n",
       "      <td>0.675480</td>\n",
       "      <td>0.555242</td>\n",
       "      <td>0.269677</td>\n",
       "      <td>0.211521</td>\n",
       "      <td>0.160328</td>\n",
       "      <td>0.289614</td>\n",
       "      <td>0.358935</td>\n",
       "      <td>0.049690</td>\n",
       "      <td>0.148488</td>\n",
       "      <td>0.198331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361082</td>\n",
       "      <td>0.341145</td>\n",
       "      <td>0.274584</td>\n",
       "      <td>0.277958</td>\n",
       "      <td>0.330102</td>\n",
       "      <td>0.410159</td>\n",
       "      <td>0.252500</td>\n",
       "      <td>0.022606</td>\n",
       "      <td>0.288080</td>\n",
       "      <td>0.064413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity_mean</th>\n",
       "      <td>0.532568</td>\n",
       "      <td>0.500469</td>\n",
       "      <td>0.186106</td>\n",
       "      <td>0.089035</td>\n",
       "      <td>0.097259</td>\n",
       "      <td>0.098430</td>\n",
       "      <td>0.180904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011729</td>\n",
       "      <td>0.101546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282099</td>\n",
       "      <td>0.261246</td>\n",
       "      <td>0.254217</td>\n",
       "      <td>0.341378</td>\n",
       "      <td>0.107029</td>\n",
       "      <td>0.201640</td>\n",
       "      <td>0.165651</td>\n",
       "      <td>0.016987</td>\n",
       "      <td>0.263824</td>\n",
       "      <td>0.055834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points_mean</th>\n",
       "      <td>0.424602</td>\n",
       "      <td>0.498012</td>\n",
       "      <td>0.148012</td>\n",
       "      <td>0.168986</td>\n",
       "      <td>0.092594</td>\n",
       "      <td>0.156660</td>\n",
       "      <td>0.305268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037689</td>\n",
       "      <td>0.088370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349950</td>\n",
       "      <td>0.321173</td>\n",
       "      <td>0.174453</td>\n",
       "      <td>0.430666</td>\n",
       "      <td>0.154573</td>\n",
       "      <td>0.142744</td>\n",
       "      <td>0.173211</td>\n",
       "      <td>0.031064</td>\n",
       "      <td>0.321223</td>\n",
       "      <td>0.087972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry_mean</th>\n",
       "      <td>0.489899</td>\n",
       "      <td>0.321212</td>\n",
       "      <td>0.072222</td>\n",
       "      <td>0.243939</td>\n",
       "      <td>0.514646</td>\n",
       "      <td>0.334848</td>\n",
       "      <td>0.307071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.264646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364646</td>\n",
       "      <td>0.593434</td>\n",
       "      <td>0.253535</td>\n",
       "      <td>0.457576</td>\n",
       "      <td>0.458081</td>\n",
       "      <td>0.425253</td>\n",
       "      <td>0.374242</td>\n",
       "      <td>0.226263</td>\n",
       "      <td>0.307576</td>\n",
       "      <td>0.342929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <td>0.683867</td>\n",
       "      <td>0.499789</td>\n",
       "      <td>0.350253</td>\n",
       "      <td>0.311710</td>\n",
       "      <td>0.204718</td>\n",
       "      <td>0.278222</td>\n",
       "      <td>0.394482</td>\n",
       "      <td>0.106571</td>\n",
       "      <td>0.217355</td>\n",
       "      <td>0.435762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206403</td>\n",
       "      <td>0.302654</td>\n",
       "      <td>0.215670</td>\n",
       "      <td>0.256318</td>\n",
       "      <td>0.382266</td>\n",
       "      <td>0.839090</td>\n",
       "      <td>0.320977</td>\n",
       "      <td>0.080034</td>\n",
       "      <td>0.326032</td>\n",
       "      <td>0.143429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius_se</th>\n",
       "      <td>0.067391</td>\n",
       "      <td>0.295999</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>0.039725</td>\n",
       "      <td>0.041861</td>\n",
       "      <td>0.031975</td>\n",
       "      <td>0.094333</td>\n",
       "      <td>0.073366</td>\n",
       "      <td>0.067391</td>\n",
       "      <td>0.105559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081695</td>\n",
       "      <td>0.111968</td>\n",
       "      <td>0.073764</td>\n",
       "      <td>0.217744</td>\n",
       "      <td>0.026688</td>\n",
       "      <td>0.150172</td>\n",
       "      <td>0.070433</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>0.039580</td>\n",
       "      <td>0.029296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture_se</th>\n",
       "      <td>0.273780</td>\n",
       "      <td>0.244165</td>\n",
       "      <td>0.237314</td>\n",
       "      <td>0.150681</td>\n",
       "      <td>0.197755</td>\n",
       "      <td>0.054500</td>\n",
       "      <td>0.106546</td>\n",
       "      <td>0.781427</td>\n",
       "      <td>0.354889</td>\n",
       "      <td>0.235104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085617</td>\n",
       "      <td>0.328147</td>\n",
       "      <td>0.094634</td>\n",
       "      <td>0.269802</td>\n",
       "      <td>0.085639</td>\n",
       "      <td>0.108734</td>\n",
       "      <td>0.286598</td>\n",
       "      <td>0.079473</td>\n",
       "      <td>0.131078</td>\n",
       "      <td>0.267592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter_se</th>\n",
       "      <td>0.060406</td>\n",
       "      <td>0.237667</td>\n",
       "      <td>0.048108</td>\n",
       "      <td>0.040381</td>\n",
       "      <td>0.032229</td>\n",
       "      <td>0.032323</td>\n",
       "      <td>0.072893</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>0.056637</td>\n",
       "      <td>0.093766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088206</td>\n",
       "      <td>0.130849</td>\n",
       "      <td>0.073270</td>\n",
       "      <td>0.194977</td>\n",
       "      <td>0.029496</td>\n",
       "      <td>0.113603</td>\n",
       "      <td>0.065872</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.039815</td>\n",
       "      <td>0.020073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_se</th>\n",
       "      <td>0.032010</td>\n",
       "      <td>0.183224</td>\n",
       "      <td>0.005131</td>\n",
       "      <td>0.018244</td>\n",
       "      <td>0.017479</td>\n",
       "      <td>0.017310</td>\n",
       "      <td>0.052369</td>\n",
       "      <td>0.029899</td>\n",
       "      <td>0.023941</td>\n",
       "      <td>0.030011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049436</td>\n",
       "      <td>0.045196</td>\n",
       "      <td>0.038472</td>\n",
       "      <td>0.156273</td>\n",
       "      <td>0.014696</td>\n",
       "      <td>0.034811</td>\n",
       "      <td>0.025809</td>\n",
       "      <td>0.007411</td>\n",
       "      <td>0.022503</td>\n",
       "      <td>0.014714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness_se</th>\n",
       "      <td>0.184791</td>\n",
       "      <td>0.171771</td>\n",
       "      <td>0.113166</td>\n",
       "      <td>0.142673</td>\n",
       "      <td>0.144678</td>\n",
       "      <td>0.091784</td>\n",
       "      <td>0.081925</td>\n",
       "      <td>0.199918</td>\n",
       "      <td>0.379950</td>\n",
       "      <td>0.412585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140259</td>\n",
       "      <td>0.311623</td>\n",
       "      <td>0.189754</td>\n",
       "      <td>0.217187</td>\n",
       "      <td>0.081042</td>\n",
       "      <td>0.526804</td>\n",
       "      <td>0.232077</td>\n",
       "      <td>0.060475</td>\n",
       "      <td>0.122412</td>\n",
       "      <td>0.114458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness_se</th>\n",
       "      <td>0.525115</td>\n",
       "      <td>0.510695</td>\n",
       "      <td>0.325563</td>\n",
       "      <td>0.134497</td>\n",
       "      <td>0.119176</td>\n",
       "      <td>0.091462</td>\n",
       "      <td>0.119701</td>\n",
       "      <td>0.049764</td>\n",
       "      <td>0.066002</td>\n",
       "      <td>0.203293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226800</td>\n",
       "      <td>0.261724</td>\n",
       "      <td>0.221843</td>\n",
       "      <td>0.140881</td>\n",
       "      <td>0.125635</td>\n",
       "      <td>0.686664</td>\n",
       "      <td>0.156578</td>\n",
       "      <td>0.010950</td>\n",
       "      <td>0.117598</td>\n",
       "      <td>0.028885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity_se</th>\n",
       "      <td>0.195530</td>\n",
       "      <td>0.166439</td>\n",
       "      <td>0.108712</td>\n",
       "      <td>0.048030</td>\n",
       "      <td>0.050682</td>\n",
       "      <td>0.038106</td>\n",
       "      <td>0.050404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>0.068737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093813</td>\n",
       "      <td>0.093131</td>\n",
       "      <td>0.116540</td>\n",
       "      <td>0.084394</td>\n",
       "      <td>0.042879</td>\n",
       "      <td>0.143207</td>\n",
       "      <td>0.074369</td>\n",
       "      <td>0.012187</td>\n",
       "      <td>0.055177</td>\n",
       "      <td>0.026995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points_se</th>\n",
       "      <td>0.271263</td>\n",
       "      <td>0.437772</td>\n",
       "      <td>0.315780</td>\n",
       "      <td>0.191514</td>\n",
       "      <td>0.133112</td>\n",
       "      <td>0.139591</td>\n",
       "      <td>0.190188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094090</td>\n",
       "      <td>0.193787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276378</td>\n",
       "      <td>0.308202</td>\n",
       "      <td>0.237545</td>\n",
       "      <td>0.303277</td>\n",
       "      <td>0.122940</td>\n",
       "      <td>0.334533</td>\n",
       "      <td>0.264823</td>\n",
       "      <td>0.068346</td>\n",
       "      <td>0.181228</td>\n",
       "      <td>0.128755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry_se</th>\n",
       "      <td>0.140823</td>\n",
       "      <td>0.124500</td>\n",
       "      <td>0.236647</td>\n",
       "      <td>0.058226</td>\n",
       "      <td>0.166573</td>\n",
       "      <td>0.079614</td>\n",
       "      <td>0.037542</td>\n",
       "      <td>0.168965</td>\n",
       "      <td>0.486126</td>\n",
       "      <td>0.350763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095514</td>\n",
       "      <td>0.522148</td>\n",
       "      <td>0.108741</td>\n",
       "      <td>0.176845</td>\n",
       "      <td>0.125204</td>\n",
       "      <td>0.246637</td>\n",
       "      <td>0.109304</td>\n",
       "      <td>0.105223</td>\n",
       "      <td>0.061181</td>\n",
       "      <td>0.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <td>0.317331</td>\n",
       "      <td>0.359479</td>\n",
       "      <td>0.223291</td>\n",
       "      <td>0.076427</td>\n",
       "      <td>0.059153</td>\n",
       "      <td>0.030824</td>\n",
       "      <td>0.080918</td>\n",
       "      <td>0.030340</td>\n",
       "      <td>0.036904</td>\n",
       "      <td>0.129320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076911</td>\n",
       "      <td>0.133811</td>\n",
       "      <td>0.080677</td>\n",
       "      <td>0.126971</td>\n",
       "      <td>0.052865</td>\n",
       "      <td>0.726725</td>\n",
       "      <td>0.101751</td>\n",
       "      <td>0.016797</td>\n",
       "      <td>0.067410</td>\n",
       "      <td>0.022014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius_worst</th>\n",
       "      <td>0.254714</td>\n",
       "      <td>0.485237</td>\n",
       "      <td>0.165066</td>\n",
       "      <td>0.217360</td>\n",
       "      <td>0.204198</td>\n",
       "      <td>0.263963</td>\n",
       "      <td>0.314123</td>\n",
       "      <td>0.141942</td>\n",
       "      <td>0.144077</td>\n",
       "      <td>0.110993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385272</td>\n",
       "      <td>0.317681</td>\n",
       "      <td>0.244397</td>\n",
       "      <td>0.509427</td>\n",
       "      <td>0.233725</td>\n",
       "      <td>0.064141</td>\n",
       "      <td>0.185343</td>\n",
       "      <td>0.248310</td>\n",
       "      <td>0.324084</td>\n",
       "      <td>0.192458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture_worst</th>\n",
       "      <td>0.763859</td>\n",
       "      <td>0.449094</td>\n",
       "      <td>0.444829</td>\n",
       "      <td>0.241471</td>\n",
       "      <td>0.343284</td>\n",
       "      <td>0.350213</td>\n",
       "      <td>0.224147</td>\n",
       "      <td>0.700426</td>\n",
       "      <td>0.366738</td>\n",
       "      <td>0.251866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265458</td>\n",
       "      <td>0.608475</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>0.343284</td>\n",
       "      <td>0.225746</td>\n",
       "      <td>0.097281</td>\n",
       "      <td>0.459488</td>\n",
       "      <td>0.230011</td>\n",
       "      <td>0.500533</td>\n",
       "      <td>0.554904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter_worst</th>\n",
       "      <td>0.235271</td>\n",
       "      <td>0.464117</td>\n",
       "      <td>0.184023</td>\n",
       "      <td>0.211116</td>\n",
       "      <td>0.186115</td>\n",
       "      <td>0.256437</td>\n",
       "      <td>0.295284</td>\n",
       "      <td>0.123413</td>\n",
       "      <td>0.131979</td>\n",
       "      <td>0.105932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367996</td>\n",
       "      <td>0.321679</td>\n",
       "      <td>0.251457</td>\n",
       "      <td>0.473081</td>\n",
       "      <td>0.227501</td>\n",
       "      <td>0.060511</td>\n",
       "      <td>0.174810</td>\n",
       "      <td>0.219284</td>\n",
       "      <td>0.316201</td>\n",
       "      <td>0.170178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_worst</th>\n",
       "      <td>0.129326</td>\n",
       "      <td>0.307658</td>\n",
       "      <td>0.074518</td>\n",
       "      <td>0.101824</td>\n",
       "      <td>0.094057</td>\n",
       "      <td>0.131390</td>\n",
       "      <td>0.165086</td>\n",
       "      <td>0.062525</td>\n",
       "      <td>0.061664</td>\n",
       "      <td>0.044681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217460</td>\n",
       "      <td>0.153878</td>\n",
       "      <td>0.123845</td>\n",
       "      <td>0.335185</td>\n",
       "      <td>0.109443</td>\n",
       "      <td>0.024381</td>\n",
       "      <td>0.082703</td>\n",
       "      <td>0.122739</td>\n",
       "      <td>0.168133</td>\n",
       "      <td>0.089117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness_worst</th>\n",
       "      <td>0.753682</td>\n",
       "      <td>0.327082</td>\n",
       "      <td>0.111074</td>\n",
       "      <td>0.551608</td>\n",
       "      <td>0.439345</td>\n",
       "      <td>0.602457</td>\n",
       "      <td>0.279535</td>\n",
       "      <td>0.141980</td>\n",
       "      <td>0.470382</td>\n",
       "      <td>0.498778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477646</td>\n",
       "      <td>0.559532</td>\n",
       "      <td>0.422175</td>\n",
       "      <td>0.522552</td>\n",
       "      <td>0.396421</td>\n",
       "      <td>0.327082</td>\n",
       "      <td>0.644720</td>\n",
       "      <td>0.095754</td>\n",
       "      <td>0.595192</td>\n",
       "      <td>0.271611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness_worst</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.437766</td>\n",
       "      <td>0.285347</td>\n",
       "      <td>0.189976</td>\n",
       "      <td>0.167855</td>\n",
       "      <td>0.276615</td>\n",
       "      <td>0.184640</td>\n",
       "      <td>0.026826</td>\n",
       "      <td>0.067332</td>\n",
       "      <td>0.144764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407981</td>\n",
       "      <td>0.367329</td>\n",
       "      <td>0.317849</td>\n",
       "      <td>0.195797</td>\n",
       "      <td>0.242852</td>\n",
       "      <td>0.209865</td>\n",
       "      <td>0.231598</td>\n",
       "      <td>0.022383</td>\n",
       "      <td>0.319692</td>\n",
       "      <td>0.059503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity_worst</th>\n",
       "      <td>0.882588</td>\n",
       "      <td>0.412540</td>\n",
       "      <td>0.232588</td>\n",
       "      <td>0.143051</td>\n",
       "      <td>0.181070</td>\n",
       "      <td>0.211981</td>\n",
       "      <td>0.148243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010663</td>\n",
       "      <td>0.103834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395847</td>\n",
       "      <td>0.299042</td>\n",
       "      <td>0.359744</td>\n",
       "      <td>0.261342</td>\n",
       "      <td>0.150958</td>\n",
       "      <td>0.114537</td>\n",
       "      <td>0.229473</td>\n",
       "      <td>0.030879</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.091454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points_worst</th>\n",
       "      <td>0.759450</td>\n",
       "      <td>0.685911</td>\n",
       "      <td>0.375258</td>\n",
       "      <td>0.396907</td>\n",
       "      <td>0.262268</td>\n",
       "      <td>0.490378</td>\n",
       "      <td>0.349828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069485</td>\n",
       "      <td>0.183299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680756</td>\n",
       "      <td>0.608935</td>\n",
       "      <td>0.405842</td>\n",
       "      <td>0.575258</td>\n",
       "      <td>0.250275</td>\n",
       "      <td>0.164467</td>\n",
       "      <td>0.418557</td>\n",
       "      <td>0.114536</td>\n",
       "      <td>0.627835</td>\n",
       "      <td>0.255361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry_worst</th>\n",
       "      <td>0.552139</td>\n",
       "      <td>0.145082</td>\n",
       "      <td>0.123398</td>\n",
       "      <td>0.161049</td>\n",
       "      <td>0.357579</td>\n",
       "      <td>0.384979</td>\n",
       "      <td>0.120639</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.340430</td>\n",
       "      <td>0.190814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286615</td>\n",
       "      <td>0.622708</td>\n",
       "      <td>0.196728</td>\n",
       "      <td>0.261975</td>\n",
       "      <td>0.319141</td>\n",
       "      <td>0.135817</td>\n",
       "      <td>0.244628</td>\n",
       "      <td>0.176030</td>\n",
       "      <td>0.318155</td>\n",
       "      <td>0.222551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.441821</td>\n",
       "      <td>0.252197</td>\n",
       "      <td>0.199987</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>0.207333</td>\n",
       "      <td>0.199725</td>\n",
       "      <td>0.026302</td>\n",
       "      <td>0.066772</td>\n",
       "      <td>0.194412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237439</td>\n",
       "      <td>0.311951</td>\n",
       "      <td>0.175128</td>\n",
       "      <td>0.193625</td>\n",
       "      <td>0.175718</td>\n",
       "      <td>0.349993</td>\n",
       "      <td>0.235668</td>\n",
       "      <td>0.040404</td>\n",
       "      <td>0.330972</td>\n",
       "      <td>0.090122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 483 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              9         468       382       322       84   \\\n",
       "radius_mean              0.259312  0.502579  0.239907  0.278243  0.237541   \n",
       "texture_mean             0.484613  0.460602  0.439973  0.122083  0.200879   \n",
       "perimeter_mean           0.277659  0.519729  0.241587  0.269712  0.229148   \n",
       "area_mean                0.140997  0.355037  0.129077  0.153256  0.127169   \n",
       "smoothness_mean          0.595558  0.363456  0.150943  0.548614  0.402636   \n",
       "compactness_mean         0.675480  0.555242  0.269677  0.211521  0.160328   \n",
       "concavity_mean           0.532568  0.500469  0.186106  0.089035  0.097259   \n",
       "concave points_mean      0.424602  0.498012  0.148012  0.168986  0.092594   \n",
       "symmetry_mean            0.489899  0.321212  0.072222  0.243939  0.514646   \n",
       "fractal_dimension_mean   0.683867  0.499789  0.350253  0.311710  0.204718   \n",
       "radius_se                0.067391  0.295999  0.002861  0.039725  0.041861   \n",
       "texture_se               0.273780  0.244165  0.237314  0.150681  0.197755   \n",
       "perimeter_se             0.060406  0.237667  0.048108  0.040381  0.032229   \n",
       "area_se                  0.032010  0.183224  0.005131  0.018244  0.017479   \n",
       "smoothness_se            0.184791  0.171771  0.113166  0.142673  0.144678   \n",
       "compactness_se           0.525115  0.510695  0.325563  0.134497  0.119176   \n",
       "concavity_se             0.195530  0.166439  0.108712  0.048030  0.050682   \n",
       "concave points_se        0.271263  0.437772  0.315780  0.191514  0.133112   \n",
       "symmetry_se              0.140823  0.124500  0.236647  0.058226  0.166573   \n",
       "fractal_dimension_se     0.317331  0.359479  0.223291  0.076427  0.059153   \n",
       "radius_worst             0.254714  0.485237  0.165066  0.217360  0.204198   \n",
       "texture_worst            0.763859  0.449094  0.444829  0.241471  0.343284   \n",
       "perimeter_worst          0.235271  0.464117  0.184023  0.211116  0.186115   \n",
       "area_worst               0.129326  0.307658  0.074518  0.101824  0.094057   \n",
       "smoothness_worst         0.753682  0.327082  0.111074  0.551608  0.439345   \n",
       "compactness_worst        1.000000  0.437766  0.285347  0.189976  0.167855   \n",
       "concavity_worst          0.882588  0.412540  0.232588  0.143051  0.181070   \n",
       "concave points_worst     0.759450  0.685911  0.375258  0.396907  0.262268   \n",
       "symmetry_worst           0.552139  0.145082  0.123398  0.161049  0.357579   \n",
       "fractal_dimension_worst  1.000000  0.441821  0.252197  0.199987  0.158730   \n",
       "\n",
       "                              526       500       561       332       110  \\\n",
       "radius_mean              0.306640  0.381419  0.199678  0.200625  0.132330   \n",
       "texture_mean             0.305715  0.237741  0.664863  0.343253  0.246195   \n",
       "perimeter_mean           0.301638  0.379656  0.185751  0.194527  0.129293   \n",
       "area_mean                0.172895  0.231559  0.102863  0.103415  0.062227   \n",
       "smoothness_mean          0.495351  0.417080  0.197346  0.476393  0.461045   \n",
       "compactness_mean         0.289614  0.358935  0.049690  0.148488  0.198331   \n",
       "concavity_mean           0.098430  0.180904  0.000000  0.011729  0.101546   \n",
       "concave points_mean      0.156660  0.305268  0.000000  0.037689  0.088370   \n",
       "symmetry_mean            0.334848  0.307071  0.000000  0.444444  0.264646   \n",
       "fractal_dimension_mean   0.278222  0.394482  0.106571  0.217355  0.435762   \n",
       "radius_se                0.031975  0.094333  0.073366  0.067391  0.105559   \n",
       "texture_se               0.054500  0.106546  0.781427  0.354889  0.235104   \n",
       "perimeter_se             0.032323  0.072893  0.060500  0.056637  0.093766   \n",
       "area_se                  0.017310  0.052369  0.029899  0.023941  0.030011   \n",
       "smoothness_se            0.091784  0.081925  0.199918  0.379950  0.412585   \n",
       "compactness_se           0.091462  0.119701  0.049764  0.066002  0.203293   \n",
       "concavity_se             0.038106  0.050404  0.000000  0.008326  0.068737   \n",
       "concave points_se        0.139591  0.190188  0.000000  0.094090  0.193787   \n",
       "symmetry_se              0.079614  0.037542  0.168965  0.486126  0.350763   \n",
       "fractal_dimension_se     0.030824  0.080918  0.030340  0.036904  0.129320   \n",
       "radius_worst             0.263963  0.314123  0.141942  0.144077  0.110993   \n",
       "texture_worst            0.350213  0.224147  0.700426  0.366738  0.251866   \n",
       "perimeter_worst          0.256437  0.295284  0.123413  0.131979  0.105932   \n",
       "area_worst               0.131390  0.165086  0.062525  0.061664  0.044681   \n",
       "smoothness_worst         0.602457  0.279535  0.141980  0.470382  0.498778   \n",
       "compactness_worst        0.276615  0.184640  0.026826  0.067332  0.144764   \n",
       "concavity_worst          0.211981  0.148243  0.000000  0.010663  0.103834   \n",
       "concave points_worst     0.490378  0.349828  0.000000  0.069485  0.183299   \n",
       "symmetry_worst           0.384979  0.120639  0.000197  0.340430  0.190814   \n",
       "fractal_dimension_worst  0.207333  0.199725  0.026302  0.066772  0.194412   \n",
       "\n",
       "                         ...       330       214       466       121  \\\n",
       "radius_mean              ...  0.428274  0.341190  0.291495  0.552747   \n",
       "texture_mean             ...  0.196145  0.476835  0.373013  0.250592   \n",
       "perimeter_mean           ...  0.428512  0.339161  0.291549  0.536314   \n",
       "area_mean                ...  0.275589  0.198176  0.166872  0.395970   \n",
       "smoothness_mean          ...  0.381692  0.379164  0.308026  0.476393   \n",
       "compactness_mean         ...  0.361082  0.341145  0.274584  0.277958   \n",
       "concavity_mean           ...  0.282099  0.261246  0.254217  0.341378   \n",
       "concave points_mean      ...  0.349950  0.321173  0.174453  0.430666   \n",
       "symmetry_mean            ...  0.364646  0.593434  0.253535  0.457576   \n",
       "fractal_dimension_mean   ...  0.206403  0.302654  0.215670  0.256318   \n",
       "radius_se                ...  0.081695  0.111968  0.073764  0.217744   \n",
       "texture_se               ...  0.085617  0.328147  0.094634  0.269802   \n",
       "perimeter_se             ...  0.088206  0.130849  0.073270  0.194977   \n",
       "area_se                  ...  0.049436  0.045196  0.038472  0.156273   \n",
       "smoothness_se            ...  0.140259  0.311623  0.189754  0.217187   \n",
       "compactness_se           ...  0.226800  0.261724  0.221843  0.140881   \n",
       "concavity_se             ...  0.093813  0.093131  0.116540  0.084394   \n",
       "concave points_se        ...  0.276378  0.308202  0.237545  0.303277   \n",
       "symmetry_se              ...  0.095514  0.522148  0.108741  0.176845   \n",
       "fractal_dimension_se     ...  0.076911  0.133811  0.080677  0.126971   \n",
       "radius_worst             ...  0.385272  0.317681  0.244397  0.509427   \n",
       "texture_worst            ...  0.265458  0.608475  0.358209  0.343284   \n",
       "perimeter_worst          ...  0.367996  0.321679  0.251457  0.473081   \n",
       "area_worst               ...  0.217460  0.153878  0.123845  0.335185   \n",
       "smoothness_worst         ...  0.477646  0.559532  0.422175  0.522552   \n",
       "compactness_worst        ...  0.407981  0.367329  0.317849  0.195797   \n",
       "concavity_worst          ...  0.395847  0.299042  0.359744  0.261342   \n",
       "concave points_worst     ...  0.680756  0.608935  0.405842  0.575258   \n",
       "symmetry_worst           ...  0.286615  0.622708  0.196728  0.261975   \n",
       "fractal_dimension_worst  ...  0.237439  0.311951  0.175128  0.193625   \n",
       "\n",
       "                              20        71        106       270       435  \\\n",
       "radius_mean              0.288655  0.090255  0.220503  0.345923  0.331251   \n",
       "texture_mean             0.202908  0.166723  0.291512  0.240446  0.335137   \n",
       "perimeter_mean           0.289130  0.103656  0.216847  0.321401  0.327068   \n",
       "area_mean                0.159703  0.042630  0.114104  0.207466  0.193425   \n",
       "smoothness_mean          0.495351  0.408053  0.555836  0.105263  0.481809   \n",
       "compactness_mean         0.330102  0.410159  0.252500  0.022606  0.288080   \n",
       "concavity_mean           0.107029  0.201640  0.165651  0.016987  0.263824   \n",
       "concave points_mean      0.154573  0.142744  0.173211  0.031064  0.321223   \n",
       "symmetry_mean            0.458081  0.425253  0.374242  0.226263  0.307576   \n",
       "fractal_dimension_mean   0.382266  0.839090  0.320977  0.080034  0.326032   \n",
       "radius_se                0.026688  0.150172  0.070433  0.006772  0.039580   \n",
       "texture_se               0.085639  0.108734  0.286598  0.079473  0.131078   \n",
       "perimeter_se             0.029496  0.113603  0.065872  0.004095  0.039815   \n",
       "area_se                  0.014696  0.034811  0.025809  0.007411  0.022503   \n",
       "smoothness_se            0.081042  0.526804  0.232077  0.060475  0.122412   \n",
       "compactness_se           0.125635  0.686664  0.156578  0.010950  0.117598   \n",
       "concavity_se             0.042879  0.143207  0.074369  0.012187  0.055177   \n",
       "concave points_se        0.122940  0.334533  0.264823  0.068346  0.181228   \n",
       "symmetry_se              0.125204  0.246637  0.109304  0.105223  0.061181   \n",
       "fractal_dimension_se     0.052865  0.726725  0.101751  0.016797  0.067410   \n",
       "radius_worst             0.233725  0.064141  0.185343  0.248310  0.324084   \n",
       "texture_worst            0.225746  0.097281  0.459488  0.230011  0.500533   \n",
       "perimeter_worst          0.227501  0.060511  0.174810  0.219284  0.316201   \n",
       "area_worst               0.109443  0.024381  0.082703  0.122739  0.168133   \n",
       "smoothness_worst         0.396421  0.327082  0.644720  0.095754  0.595192   \n",
       "compactness_worst        0.242852  0.209865  0.231598  0.022383  0.319692   \n",
       "concavity_worst          0.150958  0.114537  0.229473  0.030879  0.325000   \n",
       "concave points_worst     0.250275  0.164467  0.418557  0.114536  0.627835   \n",
       "symmetry_worst           0.319141  0.135817  0.244628  0.176030  0.318155   \n",
       "fractal_dimension_worst  0.175718  0.349993  0.235668  0.040404  0.330972   \n",
       "\n",
       "                              102  \n",
       "radius_mean              0.246060  \n",
       "texture_mean             0.365573  \n",
       "perimeter_mean           0.231014  \n",
       "area_mean                0.133701  \n",
       "smoothness_mean          0.248262  \n",
       "compactness_mean         0.064413  \n",
       "concavity_mean           0.055834  \n",
       "concave points_mean      0.087972  \n",
       "symmetry_mean            0.342929  \n",
       "fractal_dimension_mean   0.143429  \n",
       "radius_se                0.029296  \n",
       "texture_se               0.267592  \n",
       "perimeter_se             0.020073  \n",
       "area_se                  0.014714  \n",
       "smoothness_se            0.114458  \n",
       "compactness_se           0.028885  \n",
       "concavity_se             0.026995  \n",
       "concave points_se        0.128755  \n",
       "symmetry_se              0.092700  \n",
       "fractal_dimension_se     0.022014  \n",
       "radius_worst             0.192458  \n",
       "texture_worst            0.554904  \n",
       "perimeter_worst          0.170178  \n",
       "area_worst               0.089117  \n",
       "smoothness_worst         0.271611  \n",
       "compactness_worst        0.059503  \n",
       "concavity_worst          0.091454  \n",
       "concave points_worst     0.255361  \n",
       "symmetry_worst           0.222551  \n",
       "fractal_dimension_worst  0.090122  \n",
       "\n",
       "[30 rows x 483 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're just going to make the\n",
    "# entire logic regression algorithim I guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function initializes the weights (w) \n",
    "# and bias (b) for a machine learning model \n",
    "# — typically a linear model like logistic \n",
    "# regression or a simple neural network layer.\n",
    "\n",
    "# dimension - This is the number of \n",
    "# features (or input units).\n",
    "\n",
    "# w = np.random.randn(dimension, 1) * 0.01\n",
    "# This creates a weight vector w of shape (dimension, 1) filled with small random values.\n",
    "# np.random.randn(dimension, 1) generates normally-distributed random numbers (mean = 0, std = 1).\n",
    "# Multiplying by 0.01 makes the weights small, which helps in training (especially for gradient descent convergence).\n",
    "\n",
    "# Why small values?\n",
    "# - Prevents large initial activations.\n",
    "# - Helps avoid exploding gradients in deeper networks.\n",
    "# - Example: If your input data has 3 features, \n",
    "# dimension = 3.\n",
    "\n",
    "def initialize_weights_and_bias(dimension):\n",
    "    w = np.random.randn(dimension, 1) * 0.01  \n",
    "    b = 0.0\n",
    "    return w, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to get your value in between 0 and 1\n",
    "# via the sigmoid equation\n",
    "# \n",
    "# The sigmoid() function turns z (which can \n",
    "# be any number from -∞ to ∞) into a probability \n",
    "# between 0 and 1.\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Forward-Backward Propagation\n",
    "#\n",
    "# np.dot(w.T, x_train): Computes the matrix \n",
    "# multiplication of the weights and the input data.\n",
    "#\n",
    "# cost = (-1/m) * np.sum(y_train * \n",
    "# np.log(y_head) + (1 – y_train) * \n",
    "# np.log(1 – y_head)): \n",
    "# Measures the difference between the \n",
    "# predicted probability (y_head) and true \n",
    "# label (y_train).\n",
    "# \n",
    "# derivative_weight = (1/m) * np.dot(x_train, \n",
    "# (y_head – y_train).T): \n",
    "# This calculates the gradient of the cost \n",
    "# with respect to the weights w. It tells us \n",
    "# how much we need to change the weights to \n",
    "# reduce the cost.\n",
    "# \n",
    "# derivative_bias = (1/m) * np.sum(y_head – y_train): \n",
    "# \n",
    "# This computes the gradient of the cost with respect \n",
    "# to the bias b. It is simply the average of the \n",
    "# difference between predicted probabilities (y_head) \n",
    "# and actual labels (y_train).\n",
    "# \n",
    "\n",
    "def forward_backward_propagation(w, b, x_train, y_train):\n",
    "    \n",
    "    # x_train currently looks like\n",
    "    # (30, 483). This is the\n",
    "    # number of rows and columns in dimension.\n",
    "    \n",
    "    # this equation below is to get the second value\n",
    "    # (columns) of the x_train shape\n",
    "    m = x_train.shape[1] \n",
    "    \n",
    "    # this is a base equation to determine\n",
    "    # what you want to guess while\n",
    "    # taking weights and biases into\n",
    "    # account\n",
    "    # uses the dot function to\n",
    "    # get a vector of scalar (unit) number\n",
    "    # ---- According to GPT:\n",
    "    #   \"Performs the dot product between weights \n",
    "    #   and each training example to get a vector \n",
    "    #   of 'z' values — one for each example\"\n",
    "    # \n",
    "    # IE multiple transpose of weights\n",
    "    # by the trained data\n",
    "    #\n",
    "    # this leads to an array of scalar values\n",
    "    z = np.dot(w.T, x_train) + b\n",
    "    \n",
    "    # gets the z value of the equation results\n",
    "    y_head = sigmoid(z)\n",
    "    \n",
    "    # binary cross-entropy loss (also called log loss) is to do the following...\n",
    "    #\n",
    "    # The cost line is how we measure how wrong \n",
    "    # our model is across all the training examples.\n",
    "    #\n",
    "    # We're training the model to predict 0 or 1 \n",
    "    # (like/dislike, cat/not-cat, etc.), but it \n",
    "    # doesn’t say “1” or “0” outright — it gives \n",
    "    # a probability like “I’m 92% sure this is a yes.”\n",
    "    # \n",
    "    # So we have to ask:\n",
    "    #   “Okay, cool — but how bad was that guess \n",
    "    #   compared to the real answer?”\n",
    "    # \n",
    "    # That’s what the cost function is for.\n",
    "    #\n",
    "    # y_train * log(y_head) is to determine how\n",
    "    # big or small your number is in how\n",
    "    # close you were and provide a result\n",
    "    # that is massively or shortly negative\n",
    "    # based on how \"punished\" you should be\n",
    "    # for your answer in a positive (1) result\n",
    "    #\n",
    "    # if your correct answer is negative (0), instead\n",
    "    # of doing the above, you will do\n",
    "    # (1 - y_train) * np.log(1 - y_head)\n",
    "    # this provides a negative result based on\n",
    "    # how \"punished\" you should be for your answer\n",
    "    # in a empty (0) result\n",
    "    cost = (-1/m) * np.sum(y_train * np.log(y_head) + (1 - y_train) * np.log(1 - y_head))\n",
    "    \n",
    "    # these are the lines to actually now train\n",
    "    # the model by changing its weight\n",
    "    # and bias via your result\n",
    "    #\n",
    "    # y_head - y_train is the\n",
    "    # predicted result - actual_result\n",
    "    # which is used in both to see\n",
    "    # how close you got in an array-form\n",
    "    #\n",
    "    # for derivative_weight, it is to determine\n",
    "    # how correct/wrong you were based on\n",
    "    # how much each feature/attribute/x value\n",
    "    # was important to the result\n",
    "    #\n",
    "    # for derivative_bias, it is getting the average \n",
    "    # error across all examples\n",
    "    derivative_weight = (1/m) * np.dot(x_train, (y_head - y_train).T)\n",
    "    derivative_bias = (1/m) * np.sum(y_head - y_train)\n",
    "    \n",
    "    # gradients are a just the dictionary of these\n",
    "    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n",
    "    return cost, gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this update function is like an for loop\n",
    "# to determine how to update the training model\n",
    "# as you make predictions, get results, and improve\n",
    "#\n",
    "def update(w, b, x_train, y_train, learning_rate, num_iterations):\n",
    "    costs = []\n",
    "    gradients = {}\n",
    "    # this is to do the process for however many times you want\n",
    "    # you can think of each loop as one step of learning\n",
    "    # the more learning, the better\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # this determines how close you were to \n",
    "        # the accurate answer with a prediction\n",
    "        # created via your bias and weights\n",
    "        cost, grad = forward_backward_propagation(w, b, x_train, y_train)\n",
    "        \n",
    "        # you are applying the gradients based on\n",
    "        # how fast you want the tool to learn.\n",
    "        # you do this because if you just used \n",
    "        # the raw gradients (the full size of \n",
    "        # the error), you'd overshoot the solution \n",
    "        # or even cause your model to bounce around \n",
    "        # and never learn\n",
    "        w -= learning_rate * grad[\"derivative_weight\"]\n",
    "        b -= learning_rate * grad[\"derivative_bias\"]\n",
    "\n",
    "        # this is just to save your values to determine\n",
    "        # if your model is even learning in the process.\n",
    "        # the cost should go down to showcase the learning\n",
    "        # as your algorithim should make \"less mistakes\"\n",
    "        # as time goes on. and therefore it's \"punishment\"\n",
    "        # gets smaller over time.\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "\n",
    "    # parameters: your trained model — final weights & bias\n",
    "    # gradients: last update info (optional, not usually needed)\n",
    "    # costs: the history of cost, so you can plot learning progress\n",
    "    parameters = {\"weight\": w, \"bias\": b}\n",
    "    \n",
    "    return parameters, gradients, costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predict function is the final step\n",
    "# and its entire point is to ensure\n",
    "# that you can now predict values\n",
    "# for the test values you saved\n",
    "#\n",
    "def predict(w, b, x_test):\n",
    "    # gets x_test to be the\n",
    "    # column / example size\n",
    "    m = x_test.shape[1]\n",
    "    \n",
    "    # create's an empty array\n",
    "    # for you to make predictions\n",
    "    # and update\n",
    "    y_prediction = np.zeros((1, m))\n",
    "    \n",
    "    # you are repeating the same steps\n",
    "    # during training to determine the probability\n",
    "    # of your test data with the the weights\n",
    "    # and bias to get an array of values\n",
    "    # that is how certain your algorithim\n",
    "    # thinks it is\n",
    "    z = sigmoid(np.dot(w.T, x_test) + b)\n",
    "\n",
    "    # checks within the array of z values\n",
    "    # (your predictions). And if ur prediction\n",
    "    # is over 50%, ur going to choose 1\n",
    "    # otherwise you pick 0\n",
    "    for i in range(z.shape[1]):\n",
    "        y_prediction[0, i] = 1 if z[0, i] > 0.5 else 0\n",
    "\n",
    "    return y_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6932741103899059\n",
      "Cost after iteration 100: 0.6657099769562822\n",
      "Cost after iteration 200: 0.6412476960389252\n",
      "Cost after iteration 300: 0.6188879096313175\n",
      "Cost after iteration 400: 0.5982939214352647\n",
      "Cost after iteration 500: 0.5792730853521602\n",
      "Cost after iteration 600: 0.5616718505594276\n",
      "Cost after iteration 700: 0.5453547510154947\n",
      "Cost after iteration 800: 0.530199964377335\n",
      "Cost after iteration 900: 0.5160979037046398\n",
      "Cost after iteration 1000: 0.5029502424357432\n",
      "Cost after iteration 1100: 0.49066894756114304\n",
      "Cost after iteration 1200: 0.4791753006175157\n",
      "Cost after iteration 1300: 0.4683989425573705\n",
      "Cost after iteration 1400: 0.45827697245130006\n",
      "Cost after iteration 1500: 0.448753117102125\n",
      "Cost after iteration 1600: 0.43977697860715925\n",
      "Cost after iteration 1700: 0.4313033603502911\n",
      "Cost after iteration 1800: 0.42329166801184964\n",
      "Cost after iteration 1900: 0.4157053800984924\n",
      "Cost after iteration 2000: 0.40851158157242484\n",
      "Cost after iteration 2100: 0.4016805539518691\n",
      "Cost after iteration 2200: 0.3951854154653224\n",
      "Cost after iteration 2300: 0.3890018052785359\n",
      "Cost after iteration 2400: 0.3831076063544235\n",
      "Cost after iteration 2500: 0.37748270207861023\n",
      "Cost after iteration 2600: 0.3721087623440736\n",
      "Cost after iteration 2700: 0.3669690553138592\n",
      "Cost after iteration 2800: 0.36204828155991087\n",
      "Cost after iteration 2900: 0.3573324277047976\n",
      "Cost after iteration 3000: 0.3528086370720378\n",
      "Cost after iteration 3100: 0.3484650951826726\n",
      "Cost after iteration 3200: 0.3442909282247976\n",
      "Cost after iteration 3300: 0.34027611287338505\n",
      "Cost after iteration 3400: 0.3364113960544241\n",
      "Cost after iteration 3500: 0.3326882234344138\n",
      "Cost after iteration 3600: 0.32909867557747496\n",
      "Cost after iteration 3700: 0.3256354108512771\n",
      "Cost after iteration 3800: 0.3222916142826965\n",
      "Cost after iteration 3900: 0.319060951667333\n",
      "Cost after iteration 4000: 0.3159375283260325\n",
      "Cost after iteration 4100: 0.3129158519784217\n",
      "Cost after iteration 4200: 0.3099907992698845\n",
      "Cost after iteration 4300: 0.3071575855458774\n",
      "Cost after iteration 4400: 0.30441173751727185\n",
      "Cost after iteration 4500: 0.30174906850359884\n",
      "Cost after iteration 4600: 0.2991656559785919\n",
      "Cost after iteration 4700: 0.29665782117506656\n",
      "Cost after iteration 4800: 0.2942221105346107\n",
      "Cost after iteration 4900: 0.2918552788123766\n",
      "Cost after iteration 5000: 0.28955427366894837\n",
      "Cost after iteration 5100: 0.28731622160024134\n",
      "Cost after iteration 5200: 0.2851384150730217\n",
      "Cost after iteration 5300: 0.283018300748241\n",
      "Cost after iteration 5400: 0.2809534686872232\n",
      "Cost after iteration 5500: 0.27894164244703995\n",
      "Cost after iteration 5600: 0.27698066998139226\n",
      "Cost after iteration 5700: 0.2750685152721061\n",
      "Cost after iteration 5800: 0.27320325062415035\n",
      "Cost after iteration 5900: 0.2713830495639715\n",
      "Cost after iteration 6000: 0.2696061802870613\n",
      "Cost after iteration 6100: 0.26787099960610405\n",
      "Cost after iteration 6200: 0.2661759473558803\n",
      "Cost after iteration 6300: 0.2645195412154087\n",
      "Cost after iteration 6400: 0.2629003719116414\n",
      "Cost after iteration 6500: 0.2613170987724539\n",
      "Cost after iteration 6600: 0.25976844559973644\n",
      "Cost after iteration 6700: 0.25825319683613\n",
      "Cost after iteration 6800: 0.2567701940014105\n",
      "Cost after iteration 6900: 0.255318332376729\n",
      "Cost after iteration 7000: 0.2538965579168947\n",
      "Cost after iteration 7100: 0.25250386437267086\n",
      "Cost after iteration 7200: 0.25113929060665807\n",
      "Cost after iteration 7300: 0.2498019180877832\n",
      "Cost after iteration 7400: 0.24849086855072053\n",
      "Cost after iteration 7500: 0.24720530180774933\n",
      "Cost after iteration 7600: 0.24594441370161882\n",
      "Cost after iteration 7700: 0.24470743418895666\n",
      "Cost after iteration 7800: 0.24349362554463438\n",
      "Cost after iteration 7900: 0.2423022806782934\n",
      "Cost after iteration 8000: 0.24113272155496077\n",
      "Cost after iteration 8100: 0.2399842977123357\n",
      "Cost after iteration 8200: 0.23885638486792682\n",
      "Cost after iteration 8300: 0.23774838360976036\n",
      "Cost after iteration 8400: 0.2366597181648794\n",
      "Cost after iteration 8500: 0.23558983524029933\n",
      "Cost after iteration 8600: 0.23453820293150432\n",
      "Cost after iteration 8700: 0.2335043096939417\n",
      "Cost after iteration 8800: 0.232487663373318\n",
      "Cost after iteration 8900: 0.23148779029081626\n",
      "Cost after iteration 9000: 0.2305042343796433\n",
      "Cost after iteration 9100: 0.22953655636958115\n",
      "Cost after iteration 9200: 0.22858433301646014\n",
      "Cost after iteration 9300: 0.22764715637369623\n",
      "Cost after iteration 9400: 0.22672463310323782\n",
      "Cost after iteration 9500: 0.2258163838234609\n",
      "Cost after iteration 9600: 0.22492204249172126\n",
      "Cost after iteration 9700: 0.22404125581943568\n",
      "Cost after iteration 9800: 0.22317368271771115\n",
      "Cost after iteration 9900: 0.22231899377167733\n",
      "Cost after iteration 10000: 0.22147687074180453\n",
      "Cost after iteration 10100: 0.22064700609060486\n",
      "Cost after iteration 10200: 0.21982910253322166\n",
      "Cost after iteration 10300: 0.21902287261051404\n",
      "Cost after iteration 10400: 0.21822803828333212\n",
      "Cost after iteration 10500: 0.21744433054676837\n",
      "Cost after iteration 10600: 0.21667148906324413\n",
      "Cost after iteration 10700: 0.21590926181336953\n",
      "Cost after iteration 10800: 0.2151574047635786\n",
      "Cost after iteration 10900: 0.2144156815496081\n",
      "Cost after iteration 11000: 0.21368386317494403\n",
      "Cost after iteration 11100: 0.21296172772341818\n",
      "Cost after iteration 11200: 0.21224906008518407\n",
      "Cost after iteration 11300: 0.211545651695353\n",
      "Cost after iteration 11400: 0.21085130028461047\n",
      "Cost after iteration 11500: 0.21016580964117915\n",
      "Cost after iteration 11600: 0.2094889893835286\n",
      "Cost after iteration 11700: 0.20882065474327174\n",
      "Cost after iteration 11800: 0.2081606263577157\n",
      "Cost after iteration 11900: 0.20750873007157353\n",
      "Cost after iteration 12000: 0.20686479674736502\n",
      "Cost after iteration 12100: 0.20622866208406695\n",
      "Cost after iteration 12200: 0.20560016644359574\n",
      "Cost after iteration 12300: 0.20497915468473096\n",
      "Cost after iteration 12400: 0.2043654760041105\n",
      "Cost after iteration 12500: 0.20375898378394572\n",
      "Cost after iteration 12600: 0.2031595354461303\n",
      "Cost after iteration 12700: 0.20256699231243006\n",
      "Cost after iteration 12800: 0.20198121947045974\n",
      "Cost after iteration 12900: 0.20140208564516873\n",
      "Cost after iteration 13000: 0.20082946307557423\n",
      "Cost after iteration 13100: 0.20026322739649188\n",
      "Cost after iteration 13200: 0.1997032575250302\n",
      "Cost after iteration 13300: 0.19914943555162462\n",
      "Cost after iteration 13400: 0.19860164663540206\n",
      "Cost after iteration 13500: 0.19805977890367518\n",
      "Cost after iteration 13600: 0.19752372335537774\n",
      "Cost after iteration 13700: 0.19699337376826154\n",
      "Cost after iteration 13800: 0.19646862660968456\n",
      "Cost after iteration 13900: 0.19594938095082953\n",
      "Cost after iteration 14000: 0.1954355383841997\n",
      "Cost after iteration 14100: 0.19492700294424598\n",
      "Cost after iteration 14200: 0.19442368103098764\n",
      "Cost after iteration 14300: 0.19392548133649634\n",
      "Cost after iteration 14400: 0.19343231477411715\n",
      "Cost after iteration 14500: 0.1929440944103097\n",
      "Cost after iteration 14600: 0.19246073539899625\n",
      "Cost after iteration 14700: 0.19198215491830958\n",
      "Cost after iteration 14800: 0.1915082721096385\n",
      "Cost after iteration 14900: 0.191039008018875\n",
      "Cost after iteration 15000: 0.19057428553976902\n",
      "Cost after iteration 15100: 0.19011402935930483\n",
      "Cost after iteration 15200: 0.1896581659050142\n",
      "Cost after iteration 15300: 0.18920662329414606\n",
      "Cost after iteration 15400: 0.18875933128461772\n",
      "Cost after iteration 15500: 0.18831622122767458\n",
      "Cost after iteration 15600: 0.18787722602218926\n",
      "Cost after iteration 15700: 0.18744228007053337\n",
      "Cost after iteration 15800: 0.18701131923596057\n",
      "Cost after iteration 15900: 0.18658428080143946\n",
      "Cost after iteration 16000: 0.18616110342987935\n",
      "Cost after iteration 16100: 0.18574172712569426\n",
      "Cost after iteration 16200: 0.1853260931976522\n",
      "Cost after iteration 16300: 0.1849141442229612\n",
      "Cost after iteration 16400: 0.18450582401254195\n",
      "Cost after iteration 16500: 0.18410107757744357\n",
      "Cost after iteration 16600: 0.1836998510963576\n",
      "Cost after iteration 16700: 0.18330209188418875\n",
      "Cost after iteration 16800: 0.1829077483616423\n",
      "Cost after iteration 16900: 0.18251677002579\n",
      "Cost after iteration 17000: 0.18212910742157767\n",
      "Cost after iteration 17100: 0.18174471211424006\n",
      "Cost after iteration 17200: 0.1813635366625881\n",
      "Cost after iteration 17300: 0.18098553459313774\n",
      "Cost after iteration 17400: 0.18061066037504883\n",
      "Cost after iteration 17500: 0.1802388693958446\n",
      "Cost after iteration 17600: 0.17987011793788374\n",
      "Cost after iteration 17700: 0.17950436315555704\n",
      "Cost after iteration 17800: 0.17914156305318418\n",
      "Cost after iteration 17900: 0.1787816764635837\n",
      "Cost after iteration 18000: 0.17842466302729432\n",
      "Cost after iteration 18100: 0.17807048317242297\n",
      "Cost after iteration 18200: 0.17771909809509753\n",
      "Cost after iteration 18300: 0.17737046974050508\n",
      "Cost after iteration 18400: 0.17702456078449227\n",
      "Cost after iteration 18500: 0.17668133461571142\n",
      "Cost after iteration 18600: 0.17634075531829077\n",
      "Cost after iteration 18700: 0.17600278765501332\n",
      "Cost after iteration 18800: 0.17566739705098486\n",
      "Cost after iteration 18900: 0.1753345495777764\n",
      "Cost after iteration 19000: 0.17500421193802299\n",
      "Cost after iteration 19100: 0.17467635145046614\n",
      "Cost after iteration 19200: 0.17435093603542212\n",
      "Cost after iteration 19300: 0.17402793420066434\n",
      "Cost after iteration 19400: 0.1737073150277046\n",
      "Cost after iteration 19500: 0.17338904815846104\n",
      "Cost after iteration 19600: 0.17307310378229926\n",
      "Cost after iteration 19700: 0.17275945262343495\n",
      "Cost after iteration 19800: 0.17244806592868636\n",
      "Cost after iteration 19900: 0.17213891545556453\n",
      "Cost after iteration 20000: 0.17183197346069176\n",
      "Cost after iteration 20100: 0.1715272126885359\n",
      "Cost after iteration 20200: 0.17122460636045278\n",
      "Cost after iteration 20300: 0.17092412816402425\n",
      "Cost after iteration 20400: 0.1706257522426849\n",
      "Cost after iteration 20500: 0.17032945318562723\n",
      "Cost after iteration 20600: 0.1700352060179764\n",
      "Cost after iteration 20700: 0.16974298619122666\n",
      "Cost after iteration 20800: 0.16945276957393102\n",
      "Cost after iteration 20900: 0.16916453244263632\n",
      "Cost after iteration 21000: 0.16887825147305685\n",
      "Cost after iteration 21100: 0.16859390373147776\n",
      "Cost after iteration 21200: 0.16831146666638314\n",
      "Cost after iteration 21300: 0.1680309181002995\n",
      "Cost after iteration 21400: 0.16775223622185134\n",
      "Cost after iteration 21500: 0.16747539957801927\n",
      "Cost after iteration 21600: 0.16720038706659635\n",
      "Cost after iteration 21700: 0.166927177928837\n",
      "Cost after iteration 21800: 0.16665575174229114\n",
      "Cost after iteration 21900: 0.1663860884138199\n",
      "Cost after iteration 22000: 0.16611816817278638\n",
      "Cost after iteration 22100: 0.16585197156441658\n",
      "Cost after iteration 22200: 0.16558747944332586\n",
      "Cost after iteration 22300: 0.16532467296720585\n",
      "Cost after iteration 22400: 0.16506353359066728\n",
      "Cost after iteration 22500: 0.16480404305923357\n",
      "Cost after iteration 22600: 0.16454618340348254\n",
      "Cost after iteration 22700: 0.16428993693332988\n",
      "Cost after iteration 22800: 0.16403528623245173\n",
      "Cost after iteration 22900: 0.16378221415284244\n",
      "Cost after iteration 23000: 0.1635307038095025\n",
      "Cost after iteration 23100: 0.16328073857525466\n",
      "Cost after iteration 23200: 0.16303230207568314\n",
      "Cost after iteration 23300: 0.16278537818419359\n",
      "Cost after iteration 23400: 0.1625399510171895\n",
      "Cost after iteration 23500: 0.16229600492936297\n",
      "Cost after iteration 23600: 0.16205352450909552\n",
      "Cost after iteration 23700: 0.16181249457396668\n",
      "Cost after iteration 23800: 0.16157290016636716\n",
      "Cost after iteration 23900: 0.16133472654921363\n",
      "Cost after iteration 24000: 0.16109795920176298\n",
      "Cost after iteration 24100: 0.1608625838155222\n",
      "Cost after iteration 24200: 0.1606285862902526\n",
      "Cost after iteration 24300: 0.1603959527300648\n",
      "Cost after iteration 24400: 0.16016466943960292\n",
      "Cost after iteration 24500: 0.15993472292031483\n",
      "Cost after iteration 24600: 0.15970609986680678\n",
      "Cost after iteration 24700: 0.15947878716327987\n",
      "Cost after iteration 24800: 0.15925277188004613\n",
      "Cost after iteration 24900: 0.1590280412701226\n",
      "Cost after iteration 25000: 0.1588045827659007\n",
      "Cost after iteration 25100: 0.15858238397588964\n",
      "Cost after iteration 25200: 0.15836143268153113\n",
      "Cost after iteration 25300: 0.15814171683408454\n",
      "Cost after iteration 25400: 0.15792322455157992\n",
      "Cost after iteration 25500: 0.15770594411583727\n",
      "Cost after iteration 25600: 0.15748986396955092\n",
      "Cost after iteration 25700: 0.15727497271343657\n",
      "Cost after iteration 25800: 0.1570612591034395\n",
      "Cost after iteration 25900: 0.15684871204800382\n",
      "Cost after iteration 26000: 0.15663732060539903\n",
      "Cost after iteration 26100: 0.1564270739811037\n",
      "Cost after iteration 26200: 0.15621796152524503\n",
      "Cost after iteration 26300: 0.15600997273009232\n",
      "Cost after iteration 26400: 0.15580309722760335\n",
      "Cost after iteration 26500: 0.15559732478702196\n",
      "Cost after iteration 26600: 0.15539264531252628\n",
      "Cost after iteration 26700: 0.1551890488409256\n",
      "Cost after iteration 26800: 0.15498652553940528\n",
      "Cost after iteration 26900: 0.1547850657033181\n",
      "Cost after iteration 27000: 0.15458465975402133\n",
      "Cost after iteration 27100: 0.1543852982367581\n",
      "Cost after iteration 27200: 0.15418697181858196\n",
      "Cost after iteration 27300: 0.15398967128632413\n",
      "Cost after iteration 27400: 0.15379338754460165\n",
      "Cost after iteration 27500: 0.15359811161386613\n",
      "Cost after iteration 27600: 0.1534038346284915\n",
      "Cost after iteration 27700: 0.15321054783490082\n",
      "Cost after iteration 27800: 0.1530182425897297\n",
      "Cost after iteration 27900: 0.15282691035802734\n",
      "Cost after iteration 28000: 0.1526365427114925\n",
      "Cost after iteration 28100: 0.15244713132674498\n",
      "Cost after iteration 28200: 0.1522586679836311\n",
      "Cost after iteration 28300: 0.15207114456356222\n",
      "Cost after iteration 28400: 0.15188455304788626\n",
      "Cost after iteration 28500: 0.151698885516291\n",
      "Cost after iteration 28600: 0.15151413414523798\n",
      "Cost after iteration 28700: 0.15133029120642752\n",
      "Cost after iteration 28800: 0.1511473490652929\n",
      "Cost after iteration 28900: 0.15096530017952373\n",
      "Cost after iteration 29000: 0.1507841370976176\n",
      "Cost after iteration 29100: 0.15060385245745977\n",
      "Cost after iteration 29200: 0.1504244389849297\n",
      "Cost after iteration 29300: 0.15024588949253379\n",
      "Cost after iteration 29400: 0.15006819687806477\n",
      "Cost after iteration 29500: 0.14989135412328572\n",
      "Cost after iteration 29600: 0.14971535429263919\n",
      "Cost after iteration 29700: 0.14954019053198067\n",
      "Cost after iteration 29800: 0.14936585606733524\n",
      "Cost after iteration 29900: 0.1491923442036781\n",
      "Train accuracy: 96.8944099378882%\n",
      "Test accuracy: 95.34883720930233%\n"
     ]
    }
   ],
   "source": [
    "# this is the final algorithim using\n",
    "# all our parts of the algorithim\n",
    "# into 1 function called logistic regression\n",
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate=0.01, num_iterations=1000):\n",
    "    \n",
    "    # dimension is how many features/attrjbutes \n",
    "    # will be applying to the equation. (ex: age, race)\n",
    "    dimension = x_train.shape[0]\n",
    "    # this is starting the weights with a set\n",
    "    # of small values, and a bias at 0 \n",
    "    w, b = initialize_weights_and_bias(dimension)\n",
    "    \n",
    "    # this trains the model\n",
    "    parameters, gradients, costs = update(w, b, x_train, y_train, learning_rate, num_iterations)\n",
    "    \n",
    "    # they make predictions based on the testing and training data\n",
    "    # \n",
    "    # the test on training data is to determine if it remembered \n",
    "    # the original data\n",
    "    #\n",
    "    # and the test on test data is to see if it\n",
    "    # is actually able to make good predictions\n",
    "    y_prediction_test = predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n",
    "    y_prediction_train = predict(parameters[\"weight\"], parameters[\"bias\"], x_train)\n",
    "    \n",
    "    # this prints the accuracy\n",
    "    print(f\"Train accuracy: {100 - np.mean(np.abs(y_prediction_train - y_train)) * 100}%\")\n",
    "    print(f\"Test accuracy: {100 - np.mean(np.abs(y_prediction_test - y_test)) * 100}%\")\n",
    "\n",
    "logistic_regression(x_train, y_train, x_test, y_test, learning_rate=0.01, num_iterations=30000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
